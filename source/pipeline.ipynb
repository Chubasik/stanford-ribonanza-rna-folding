{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4d7e6-bdc3-43ee-b077-ab0d681fcbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "from datasets import load_from_disk\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "import wandb\n",
    "from arnie.mfe import mfe\n",
    "from arnie.bpps import bpps\n",
    "from typing import Tuple, Sequence\n",
    "import subprocess\n",
    "import pickle\n",
    "from modelling.modelling import BertForTokenRegression\n",
    "from modelling.collator import DataCollatorForTokenRegression\n",
    "from modelling.utils import load_dict_from_file\n",
    "from transformers.models.bert.modeling_bert import BertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9707e8f-d707-47eb-8807-9319c90b5167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c555d1-2cb4-4c56-a60e-509662c128b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input'\n",
    "OUTPUT_DIR = '../output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c595f52-5f13-4856-bad6-402acbb1c005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_data = load_from_disk(f\"{INPUT_DIR}/stanford-ribonanza-rna-folding/train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4a4a7-45e5-4f48-b484-8a3da19432e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reactivity_columns_to_array(row, filter_substr_and_output_keys: Sequence[Tuple[str, str]]):\n",
    "    output = {}\n",
    "    for filter_substr, output_key in filter_substr_and_output_keys:\n",
    "        output[output_key] = np.array([row[k] for k in row if filter_substr in k][:len(row['sequence'])], dtype=np.float32)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b20268-95f8-498c-b1b9-be5ea199dc1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_data = dataset_data.map(\n",
    "    reactivity_columns_to_array,\n",
    "    fn_kwargs={\n",
    "        'filter_substr_and_output_keys': (('reactivity_0', 'reactivity'), ('reactivity_error_0', 'reactivity_error'))\n",
    "    },\n",
    "    num_proc=14,\n",
    "    remove_columns=[k for k in dataset_data.features if any(x in k for x in ('reactivity_0', 'reactivity_error_0'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe2faa-9924-4311-9270-7f739346166c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_data = dataset_data.map(lambda row: {'stn_nts': np.array(row['reactivity'], dtype=np.float32) / np.array(row['reactivity_error'], dtype=np.float32)}, num_proc=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288969f8-3278-44cf-8eb4-05f30d4e9929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def min_val_ignore_nan(list_of_lists):\n",
    "    # Flatten the list of lists and ignore NaN values\n",
    "    flat_list = [item for sublist in list_of_lists for item in sublist if not math.isnan(item)]\n",
    "    \n",
    "    # Return the minimum value\n",
    "    return min(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d504865-55bc-4314-a09f-b77dbcfab20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_data_2A3_MaP = dataset_data.filter(lambda x: x['experiment_type'] == '2A3_MaP', num_proc=14)\n",
    "dataset_data_DMS_MaP = dataset_data.filter(lambda x: x['experiment_type'] == 'DMS_MaP', num_proc=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d2ddd-2728-4d7a-bebb-f1525e80ecce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert dataset_data_2A3_MaP['sequence_id'] == dataset_data_DMS_MaP['sequence_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae005e72-c4eb-45a3-8a83-09c7a31e6e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_suffix_to_columns(dataset, suffix, exceptions):\n",
    "    assert isinstance(exceptions, set)\n",
    "    for col in dataset.column_names:\n",
    "        if col in exceptions:\n",
    "            continue\n",
    "        dataset = dataset.rename_column(col, f\"{col}_{suffix}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71497283-02ad-4fb6-beb1-22e94631bc84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "do_not_rename = set(['sequence_id', 'sequence'])\n",
    "dataset_data_2A3_MaP = add_suffix_to_columns(dataset_data_2A3_MaP, \"2A3_MaP\", do_not_rename)\n",
    "dataset_data_DMS_MaP = add_suffix_to_columns(dataset_data_DMS_MaP, \"DMS_MaP\", do_not_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12d2b8-625a-4ecd-a1d2-18b769902fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_data = datasets.concatenate_datasets([dataset_data_2A3_MaP, dataset_data_DMS_MaP.remove_columns(['sequence_id', 'sequence'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10821bb3-9a47-463a-bc23-522a60d4a68b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../input/huggingface/SpliceBERT/SpliceBERT.510nt/\"\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "MAX_LEN = 207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49f9cd-f1ab-4f1e-99a5-40c2d0275466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOKENIZER.tokenize(' '.join(list('AGUG'.upper().replace(\"U\", \"T\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5665e-d8f1-43f8-ade5-68fc192a8b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(row, tokenizer, max_len, is_train=True):\n",
    "    seq = row['sequence']\n",
    "    seq = ' '.join(list(seq.upper().replace(\"U\", \"T\"))) # U -> T and add whitespace\n",
    "    preprocessed_row = tokenizer(seq, truncation=True, max_length=max_len, add_special_tokens=False)\n",
    "    if 'reactivity_2A3_MaP' in row:\n",
    "        assert 'reactivity_DMS_MaP' in row\n",
    "        if is_train:\n",
    "            labels = np.array([\n",
    "                [x if row['stn_nts_2A3_MaP'][idx] > -10000.0 else np.nan for idx, x in enumerate(row['reactivity_2A3_MaP'])][:max_len],\n",
    "                [x if row['stn_nts_DMS_MaP'][idx] > -10000.0 else np.nan for idx, x in enumerate(row['reactivity_DMS_MaP'])][:max_len]\n",
    "            ]).T\n",
    "        else:\n",
    "            labels = np.array([\n",
    "                [x if x is not None else np.nan for idx, x in enumerate(row['reactivity_2A3_MaP'])][:max_len],\n",
    "                [x if x is not None else np.nan for idx, x in enumerate(row['reactivity_DMS_MaP'])][:max_len]\n",
    "            ]).T\n",
    "        labels = np.clip(labels, 0, 1)\n",
    "        labels = labels.tolist()\n",
    "        preprocessed_row['labels'] = labels\n",
    "    else:\n",
    "        preprocessed_row['labels'] = np.zeros((len(preprocessed_row['input_ids']), 2)).tolist()\n",
    "    preprocessed_row['len'] = len(preprocessed_row['input_ids'])\n",
    "    \n",
    "    return preprocessed_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb2f64-107a-4d4d-a074-72e9b56b402c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"From https://github.com/huggingface/transformers/blob/v4.34.1/src/transformers/models/transfo_xl/modeling_transfo_xl.py#L178\n",
    "    \"\"\"\n",
    "    def __init__(self, demb):\n",
    "        super().__init__()\n",
    "\n",
    "        self.demb = demb\n",
    "\n",
    "        inv_freq = 1 / (10000 ** (torch.arange(0.0, demb, 2.0) / demb))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, pos_seq, bsz=None):\n",
    "        sinusoid_inp = torch.outer(pos_seq, self.inv_freq)\n",
    "        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n",
    "\n",
    "        if bsz is not None:\n",
    "            return pos_emb[:, None, :].expand(-1, bsz, -1)\n",
    "        else:\n",
    "            return pos_emb[:, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca59ed5-700f-4477-bfc1-534366f6edc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=9000)\n",
    "splits = folds.split(dataset_data)\n",
    "train_idx, val_idx = next(iter(splits))\n",
    "# train_idx = train_idx[:10000]\n",
    "dataset_train = dataset_data.select(train_idx)\n",
    "dataset_val = dataset_data.select(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151a4f3-1219-40bd-b39e-b1df54da61f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# leave only the most reliable sequences\n",
    "dataset_move_to_train = dataset_val.filter(lambda x: not ((x['SN_filter_2A3_MaP'] > 0) & (x['SN_filter_DMS_MaP'] > 0)), num_proc=14)\n",
    "dataset_val = dataset_val.filter(lambda x: ((x['SN_filter_2A3_MaP'] > 0) & (x['SN_filter_DMS_MaP'] > 0)), num_proc=14)\n",
    "dataset_train = datasets.concatenate_datasets([dataset_train, dataset_move_to_train])\n",
    "DATASET_LEN = len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ee548-039a-4f5e-a834-671da2053f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(dataset_move_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a7114e-a890-4279-bfcc-b1fe9be7f208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.map(\n",
    "    lambda x: {\n",
    "        'sn_2a3_map': np.nanmean(x['reactivity_2A3_MaP']) / np.nanmean(x['reactivity_error_2A3_MaP']),\n",
    "        'sn_dms_map': np.nanmean(x['reactivity_DMS_MaP']) / np.nanmean(x['reactivity_error_DMS_MaP'])\n",
    "    },\n",
    "    num_proc=14\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9a72c-c741-4fd9-b3ef-a017f799e09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_val = dataset_val.map(\n",
    "    lambda x: {\n",
    "        'sn_2a3_map': np.nanmean(x['reactivity_2A3_MaP']) / np.nanmean(x['reactivity_error_2A3_MaP']),\n",
    "        'sn_dms_map': np.nanmean(x['reactivity_DMS_MaP']) / np.nanmean(x['reactivity_error_DMS_MaP'])\n",
    "    },\n",
    "    num_proc=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e505bb1-efe5-43e3-833c-05c9a718c3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "THRESHOLD_START = 0.75\n",
    "dataset_train = dataset_train.filter(lambda x: (x['sn_2a3_map'] >= THRESHOLD_START) | (x['sn_dms_map'] >= THRESHOLD_START), num_proc=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0674583-814d-4c11-a2f1-3d2f80a09e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773245d-0737-47d0-8e7d-cb78500fa49f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.map(\n",
    "    preprocess,\n",
    "    fn_kwargs={'tokenizer': TOKENIZER, 'max_len': MAX_LEN, 'is_train': True},\n",
    "    num_proc=14,\n",
    ")\n",
    "\n",
    "dataset_val = dataset_val.map(\n",
    "    preprocess,\n",
    "    fn_kwargs={'tokenizer': TOKENIZER, 'max_len': MAX_LEN, 'is_train': False},\n",
    "    num_proc=14,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142319a-effd-4948-beea-b51f6a5ff767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_CapR(example, output_dir, column_suffix='', max_seq_len=1024, cache_dir=None):\n",
    "    \"\"\"From https://www.kaggle.com/code/ratthachat/preprocessing-deep-learning-input-from-rna-string/notebook\n",
    "    \"\"\"\n",
    "    if cache_dir is not None:\n",
    "        seq_id = example['sequence_id']\n",
    "        dir_path = f'{cache_dir}/{column_suffix}/{seq_id[0]}/{seq_id[1]}/{seq_id[2]}/{seq_id[3]}'\n",
    "        file_path = os.path.join(dir_path, f'{seq_id}.pickle')\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                cached_dict = pickle.load(f)\n",
    "            return cached_dict\n",
    "\n",
    "    rna_id = example['sequence_id']\n",
    "    rna_string = example['sequence']\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    in_file = f'{output_dir}/{rna_id}.fa'\n",
    "    out_file = f'{output_dir}/{rna_id}.out'\n",
    "\n",
    "    fp = open(in_file, \"w\")\n",
    "    fp.write('>%s\\n' % rna_id)\n",
    "    fp.write(rna_string)\n",
    "    fp.close()\n",
    "\n",
    "    subprocess.run('CapR %s %s %d' % (in_file, out_file, max_seq_len),\n",
    "                   shell=True,capture_output=False)\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        out_file,\n",
    "        skiprows=1,\n",
    "        header=None,\n",
    "        delim_whitespace=True,\n",
    "    )\n",
    "    df2 = df.T[1:]\n",
    "    df2.columns = df.T.iloc[0].values\n",
    "    os.remove(in_file)\n",
    "    os.remove(out_file)\n",
    "    \n",
    "    res = {f'capr_structure_probs{column_suffix}': df2.values.astype(np.float32).tolist()}\n",
    "    \n",
    "    if cache_dir is not None:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(res, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            if os.path.exists(file_path):\n",
    "                print(f'removing {file_path}')\n",
    "                os.remove(file_path)\n",
    "            raise e\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92299e-9567-4d72-8372-3c2491b678b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.map(\n",
    "    run_CapR,\n",
    "    fn_kwargs={\n",
    "        'output_dir': '../output/CapR',\n",
    "        'cache_dir': '../output/CapR_cache',\n",
    "    },\n",
    "    num_proc=14\n",
    ")\n",
    "\n",
    "dataset_val = dataset_val.map(\n",
    "    run_CapR,\n",
    "    fn_kwargs={\n",
    "        'output_dir': '../output/CapR',\n",
    "        'cache_dir': '../output/CapR_cache',\n",
    "    },\n",
    "    num_proc=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be723e5-5d23-45ff-911d-9f97552401cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_arnie_bpp(example, column_suffix='', cache_dir=None, file_path_only=False, **kwargs):\n",
    "    if file_path_only:\n",
    "        assert cache_dir is not None\n",
    "    if cache_dir is not None:\n",
    "        seq_id = example['sequence_id']\n",
    "        dir_path = f'{cache_dir}/{column_suffix}/{seq_id[0]}/{seq_id[1]}/{seq_id[2]}/{seq_id[3]}'\n",
    "        file_path = os.path.join(dir_path, f'{seq_id}.pickle')\n",
    "        if os.path.exists(file_path):\n",
    "            if file_path_only:\n",
    "                return {f\"file_path{column_suffix}\": file_path}\n",
    "            with open(file_path, 'rb') as f:\n",
    "                cached_dict = pickle.load(f)\n",
    "            return cached_dict\n",
    "\n",
    "    non_sparse = bpps(example['sequence'], **kwargs)\n",
    "    indices = [x.tolist() for x in np.nonzero(non_sparse)]\n",
    "    values = non_sparse[non_sparse != 0].tolist()\n",
    "    res = {\n",
    "        f\"indices{column_suffix}\": indices,\n",
    "        f\"values{column_suffix}\": values\n",
    "    }\n",
    "    if cache_dir is not None:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(res, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            if os.path.exists(file_path):\n",
    "                print(f'removing {file_path}')\n",
    "                os.remove(file_path)\n",
    "            raise e\n",
    "    if file_path_only:\n",
    "        return {f\"file_path{column_suffix}\": file_path}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41549ff0-fe28-4ac2-b77d-1fdac47ab5ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bpps(example, column_suffix='', cache_dir=None, file_path_only=False, **kwargs):\n",
    "    if file_path_only:\n",
    "        assert cache_dir is not None\n",
    "    if cache_dir is not None:\n",
    "        seq_id = example['sequence_id']\n",
    "        dir_path = f'{cache_dir}/{column_suffix}/{seq_id[0]}/{seq_id[1]}/{seq_id[2]}/{seq_id[3]}'\n",
    "        file_path = os.path.join(dir_path, f'{seq_id}.pickle')\n",
    "        if os.path.exists(file_path):\n",
    "            if file_path_only:\n",
    "                return {f\"file_path{column_suffix}\": file_path}\n",
    "            with open(file_path, 'rb') as f:\n",
    "                cached_dict = pickle.load(f)\n",
    "            return cached_dict\n",
    "\n",
    "    mfes = [\n",
    "        get_arnie_bpp(example, package='eternafold', column_suffix='', cache_dir=None, **kwargs),\n",
    "        # get_arnie_bpp(example, package='vienna_2', column_suffix='', cache_dir=None, **kwargs),\n",
    "        # get_arnie_bpp(example, package='contrafold_2', column_suffix='', cache_dir=None, **kwargs),\n",
    "        # get_arnie_bpp(example, package='rnastructure', column_suffix='', cache_dir=None, **kwargs),\n",
    "        # get_arnie_bpp(example, package='rnasoft_07', column_suffix='', cache_dir=None, **kwargs)\n",
    "    ]\n",
    "    sequence_length = len(example['sequence'])\n",
    "    adj_matrix = torch.sparse_coo_tensor(**mfes[0], size=(sequence_length, sequence_length))\n",
    "    for idx in range(1, len(mfes)):\n",
    "        adj_matrix += torch.sparse_coo_tensor(**mfes[idx], size=(sequence_length, sequence_length))\n",
    "    adj_matrix = adj_matrix.coalesce()\n",
    "    adj_matrix /= len(mfes)\n",
    "    indices = adj_matrix.indices().tolist()\n",
    "    values = adj_matrix.values().tolist()\n",
    "    res = {\n",
    "        f\"indices{column_suffix}\": indices,\n",
    "        f\"values{column_suffix}\": values\n",
    "    }\n",
    "    if cache_dir is not None:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(res, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            if os.path.exists(file_path):\n",
    "                print(f'removing {file_path}')\n",
    "                os.remove(file_path)\n",
    "            raise e\n",
    "    if file_path_only:\n",
    "        return {f\"file_path{column_suffix}\": file_path}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08a4c4-6369-4aa9-bbc3-8b807571c533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(paren_string):\n",
    "    stack = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    \n",
    "    for i, char in enumerate(paren_string):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                open_index = stack.pop()\n",
    "                rows.append(open_index)\n",
    "                cols.append(i)\n",
    "    \n",
    "    indices = torch.tensor([rows, cols], dtype=torch.long)\n",
    "    values = torch.ones(len(rows), dtype=torch.float32)\n",
    "\n",
    "    size = len(paren_string)\n",
    "    sparse_tensor = torch.sparse_coo_tensor(indices, values, (size, size)).coalesce()\n",
    "\n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb691ba6-b639-4bf4-8e8a-449c00fa4b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_arnie_mfe(example, column_suffix='', cache_dir=None, file_path_only=False, **kwargs):\n",
    "    if file_path_only:\n",
    "        assert cache_dir is not None\n",
    "    if cache_dir is not None:\n",
    "        seq_id = example['sequence_id']\n",
    "        dir_path = f'{cache_dir}/{column_suffix}/{seq_id[0]}/{seq_id[1]}/{seq_id[2]}/{seq_id[3]}'\n",
    "        file_path = os.path.join(dir_path, f'{seq_id}.pickle')\n",
    "        if os.path.exists(file_path):\n",
    "            if file_path_only:\n",
    "                return {f\"file_path{column_suffix}\": file_path}\n",
    "            with open(file_path, 'rb') as f:\n",
    "                cached_dict = pickle.load(f)\n",
    "            return cached_dict\n",
    "\n",
    "    mfe_string = mfe(example['sequence'], **kwargs)\n",
    "    adj_matrix = create_adjacency_matrix(mfe_string)\n",
    "    indices = adj_matrix.indices().tolist()\n",
    "    values = adj_matrix.values().tolist()\n",
    "    res = {\n",
    "        f\"indices{column_suffix}\": indices,\n",
    "        f\"values{column_suffix}\": values\n",
    "    }\n",
    "    if cache_dir is not None:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(res, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            if os.path.exists(file_path):\n",
    "                print(f'removing {file_path}')\n",
    "                os.remove(file_path)\n",
    "            raise e\n",
    "    if file_path_only:\n",
    "        return {f\"file_path{column_suffix}\": file_path}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41139305-95a7-4a8e-9eb8-464fc518d28a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mfes(example, cache_dir=None, column_suffix='', file_path_only=False, **kwargs):\n",
    "    if file_path_only:\n",
    "        assert cache_dir is not None\n",
    "    if cache_dir is not None:\n",
    "        seq_id = example['sequence_id']\n",
    "        dir_path = f'{cache_dir}/{column_suffix}/{seq_id[0]}/{seq_id[1]}/{seq_id[2]}/{seq_id[3]}'\n",
    "        file_path = os.path.join(dir_path, f'{seq_id}.pickle')\n",
    "        if os.path.exists(file_path):\n",
    "            if file_path_only:\n",
    "                return {f\"file_path{column_suffix}\": file_path}\n",
    "            with open(file_path, 'rb') as f:\n",
    "                cached_dict = pickle.load(f)\n",
    "            return cached_dict\n",
    "\n",
    "    mfes = [\n",
    "        get_arnie_mfe(example, package='eternafold', column_suffix='', cache_dir=None, **kwargs),\n",
    "        # get_arnie_mfe(example, package='vienna_2', column_suffix='', cache_dir=None, **kwargs),\n",
    "        # get_arnie_mfe(example, package='contrafold_2', column_suffix='', cache_dir=None, **kwargs),\n",
    "        # get_arnie_mfe(example, package='rnastructure', column_suffix='', cache_dir=None, **kwargs)\n",
    "    ]\n",
    "    sequence_length = len(example['sequence'])\n",
    "    adj_matrix = torch.sparse_coo_tensor(**mfes[0], size=(sequence_length, sequence_length))\n",
    "    for idx in range(1, len(mfes)):\n",
    "        adj_matrix += torch.sparse_coo_tensor(**mfes[idx], size=(sequence_length, sequence_length))\n",
    "    adj_matrix = adj_matrix.coalesce()\n",
    "    adj_matrix /= len(mfes)\n",
    "    indices = adj_matrix.indices().tolist()\n",
    "    values = adj_matrix.values().tolist()\n",
    "    res = {\n",
    "        f\"indices{column_suffix}\": indices,\n",
    "        f\"values{column_suffix}\": values\n",
    "    }\n",
    "    if cache_dir is not None:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(res, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            if os.path.exists(file_path):\n",
    "                print(f'removing {file_path}')\n",
    "                os.remove(file_path)\n",
    "            raise e\n",
    "    if file_path_only:\n",
    "        return {f\"file_path{column_suffix}\": file_path}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d4ad2-9eab-4505-acef-a3ee9aa30271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mfe_shortcuts(paren_string):\n",
    "    stack = []\n",
    "    result = []\n",
    "    \n",
    "    for i, char in enumerate(paren_string):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                open_index = stack.pop()\n",
    "                result.append((open_index, i))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cdb64e-44eb-422f-b20c-c481b10440d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_distance_matrix(n, shortcuts):\n",
    "    matrix = np.full((n, n), np.inf)\n",
    "    \n",
    "    np.fill_diagonal(matrix, 0)\n",
    "    for i in range(n - 1):\n",
    "        matrix[i][i + 1] = 1\n",
    "        matrix[i + 1][i] = 1\n",
    "\n",
    "    for shortcut in shortcuts:\n",
    "        start, end = shortcut\n",
    "        matrix[start][end] = 1\n",
    "        matrix[end][start] = 1\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3907a67-ece0-444c-ae9d-f741a321b6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def floyd_warshall_vectorized(matrix):\n",
    "    n = len(matrix)\n",
    "    for k in range(n):\n",
    "        matrix = np.minimum(matrix, matrix[np.newaxis, k, :] + matrix[:, k, np.newaxis])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12f812-ab8f-455b-9cb3-2294237b5deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_distance_matrix_mfe(mfe_str, size=None):\n",
    "    if size is None:\n",
    "        size = len(mfe_str)\n",
    "    distance_matrix = create_distance_matrix(size, create_mfe_shortcuts(mfe_str))\n",
    "    shortest_paths_matrix = floyd_warshall_vectorized(distance_matrix)\n",
    "    return shortest_paths_matrix.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350dd73-ef3e-4222-87f6-cbd48c73d4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sparse_distance_matrix(mfe_str):\n",
    "    matrix = get_distance_matrix_mfe(mfe_str)\n",
    "    n = matrix.shape[0]\n",
    "    rows = []\n",
    "    cols = []\n",
    "    values = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if matrix[i][j] != abs(j - i):\n",
    "                rows.append(i)\n",
    "                cols.append(j)\n",
    "                values.append(matrix[i][j])\n",
    "\n",
    "    res = {\n",
    "        f\"indices\": [rows, cols],\n",
    "        f\"values\": values\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae9251-e997-40da-acd4-25c33fdbd46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_matrix(indices, values, size, base_matrix=None, sign=False):\n",
    "    if base_matrix is not None:\n",
    "        matrix = base_matrix.copy()\n",
    "    else:\n",
    "        if sign:\n",
    "            matrix = np.fromfunction(lambda i, j: (i - j), (size, size)) # negative above the diagonal\n",
    "        else:\n",
    "            matrix = np.fromfunction(lambda i, j: abs(j - i), (size, size))\n",
    "    matrix = torch.LongTensor(matrix)\n",
    "    \n",
    "    for i, j, value in zip(indices[0], indices[1], values):\n",
    "        if sign:\n",
    "            matrix[i][j] = -value\n",
    "        else:\n",
    "            matrix[i][j] = value\n",
    "        matrix[j][i] = value\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ca437-3bbf-4646-b527-be204e6db0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_arnie_mfe_distance(example, column_suffix='', cache_dir=None, file_path_only=False, **kwargs):\n",
    "    if file_path_only:\n",
    "        assert cache_dir is not None\n",
    "    if cache_dir is not None:\n",
    "        seq_id = example['sequence_id']\n",
    "        dir_path = f'{cache_dir}/{column_suffix}/{seq_id[0]}/{seq_id[1]}/{seq_id[2]}/{seq_id[3]}'\n",
    "        file_path = os.path.join(dir_path, f'{seq_id}.pickle')\n",
    "        if os.path.exists(file_path):\n",
    "            if file_path_only:\n",
    "                return {f\"file_path{column_suffix}\": file_path}\n",
    "            with open(file_path, 'rb') as f:\n",
    "                cached_dict = pickle.load(f)\n",
    "            return cached_dict\n",
    "\n",
    "    mfe_string = mfe(example['sequence'], **kwargs)\n",
    "    distance_matrix = get_sparse_distance_matrix(mfe_string)\n",
    "    mfe_mapping = {\n",
    "        # 0 is resereved for padding\n",
    "        '.': 1,\n",
    "        '(': 2,\n",
    "        ')': 3,\n",
    "        '[': 4,\n",
    "        ']': 5\n",
    "    }\n",
    "    \n",
    "    res = {\n",
    "        f\"input_ids{column_suffix}\": [mfe_mapping[c] for c in mfe_string],\n",
    "        f\"indices{column_suffix}\": distance_matrix['indices'],\n",
    "        f\"values{column_suffix}\": distance_matrix['values']\n",
    "    }\n",
    "    if cache_dir is not None:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(res, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            if os.path.exists(file_path):\n",
    "                print(f'removing {file_path}')\n",
    "                os.remove(file_path)\n",
    "            raise e\n",
    "    if file_path_only:\n",
    "        return {f\"file_path{column_suffix}\": file_path}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18b7b9-b454-4842-8c26-1f533236390b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_predicted_loop_type(id, sequence, structure, debug=False):\n",
    "    pid = os.getpid()\n",
    "    tmp_in_file = f'/home/rapids/notebooks/host/output/tmp/{id}_{pid}.dbn'\n",
    "    tmp_out_file = f'/home/rapids/notebooks/host/output/tmp/{id}_{pid}.st'\n",
    "    with open(tmp_in_file, 'w') as file:\n",
    "        file.write(sequence + '\\n')\n",
    "        file.write(structure + '\\n')\n",
    "    perl_script_path = '/home/rapids/notebooks/toolkits/bpRNA/bpRNA.pl'\n",
    "    working_directory = '../output/tmp/'\n",
    "    subprocess.run(['perl', perl_script_path, tmp_in_file], cwd=working_directory)\n",
    "    result = [l.strip('\\n') for l in open(tmp_out_file)]\n",
    "    if debug:\n",
    "        print(sequence)\n",
    "        print(structure)\n",
    "        print(result[5])\n",
    "    else:\n",
    "        os.remove(tmp_in_file)\n",
    "        os.remove(tmp_out_file)\n",
    "    return id, structure, result[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6875c-b22d-484f-a8cb-f06fe06b4bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_arnie_predicted_loop_type(example, column_suffix='', cache_dir=None, **kwargs):\n",
    "    if cache_dir is not None:\n",
    "        seq_id = example['sequence_id']\n",
    "        dir_path = f'{cache_dir}/{column_suffix}/{seq_id[0]}/{seq_id[1]}/{seq_id[2]}/{seq_id[3]}'\n",
    "        file_path = os.path.join(dir_path, f'{seq_id}.pickle')\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                cached_dict = pickle.load(f)\n",
    "            return cached_dict\n",
    "\n",
    "    mfe_string = mfe(example['sequence'], **kwargs)\n",
    "    _, _, structure = get_predicted_loop_type(example['sequence_id'], example['sequence'], mfe_string)\n",
    "    structure_mapping = {\n",
    "        # 0 is resereved for padding\n",
    "        'S': 1, # paired \"Stem\"\n",
    "        'M': 2, # Multiloop\n",
    "        'I': 3, # Internal loop\n",
    "        'B': 4, # Bulge\n",
    "        'H': 5, # Hairpin loop\n",
    "        'K': 6, # pseudoKnot\n",
    "        'E': 7, # dangling End\n",
    "        'X': 8, # eXternal loop\n",
    "    }\n",
    "    \n",
    "    res = {\n",
    "        f\"structure{column_suffix}\": [structure_mapping[c] for c in structure]\n",
    "    }\n",
    "    if cache_dir is not None:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(res, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            if os.path.exists(file_path):\n",
    "                print(f'removing {file_path}')\n",
    "                os.remove(file_path)\n",
    "            raise e\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7aac7-f962-46ae-99cb-60df6db3862c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.map(\n",
    "    get_bpps,\n",
    "    fn_kwargs={\n",
    "        'cache_dir': '../output/arnie/eternafold',\n",
    "        'column_suffix': '_bpp',\n",
    "        'file_path_only': True,\n",
    "    },\n",
    "    num_proc=14\n",
    ")\n",
    "dataset_val = dataset_val.map(\n",
    "    get_bpps,\n",
    "    fn_kwargs={\n",
    "        'cache_dir': '../output/arnie/eternafold',\n",
    "        'column_suffix': '_bpp',\n",
    "        'file_path_only': True,\n",
    "    },\n",
    "    num_proc=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9a9bd-4016-4d43-9cb6-234d3544d18e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.map(\n",
    "    get_arnie_mfe_distance,\n",
    "    fn_kwargs={\n",
    "        'cache_dir': '../output/arnie/eternafold',\n",
    "        'package': 'eternafold',\n",
    "        'column_suffix': '_mfe_distance',\n",
    "        'file_path_only': True,\n",
    "    },\n",
    "    num_proc=14\n",
    ")\n",
    "dataset_val = dataset_val.map(\n",
    "    get_arnie_mfe_distance,\n",
    "    fn_kwargs={\n",
    "        'cache_dir': '../output/arnie/eternafold',\n",
    "        'package': 'eternafold',\n",
    "        'column_suffix': '_mfe_distance',\n",
    "        'file_path_only': True,\n",
    "    },\n",
    "    num_proc=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b523f52-2969-4730-891e-0aeca12cd44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.map(\n",
    "    get_arnie_predicted_loop_type,\n",
    "    fn_kwargs={\n",
    "        'cache_dir': '../output/arnie/eternafold_bprna',\n",
    "        'package': 'eternafold',\n",
    "        'column_suffix': '_eternafold',\n",
    "    },\n",
    "    num_proc=14\n",
    ")\n",
    "dataset_val = dataset_val.map(\n",
    "    get_arnie_predicted_loop_type,\n",
    "    fn_kwargs={\n",
    "        'cache_dir': '../output/arnie/eternafold_bprna',\n",
    "        'package': 'eternafold',\n",
    "        'column_suffix': '_eternafold',\n",
    "    },\n",
    "    num_proc=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70c1f4-4b05-4dbc-8061-c9618ecc155e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "COLS_TO_KEEP = (\n",
    "    'input_ids',\n",
    "    'token_type_ids',\n",
    "    'attention_mask',\n",
    "    'labels',\n",
    "    'file_path_bpp',\n",
    "    'file_path_mfe_distance',\n",
    "    'structure_eternafold',\n",
    "    'sn_2a3_map',\n",
    "    'sn_dms_map',\n",
    "    'stn_nts_2A3_MaP',\n",
    "    'stn_nts_DMS_MaP',\n",
    "    'capr_structure_probs',\n",
    "    'len',\n",
    ")\n",
    "cols_to_remove = [x for x in dataset_train.column_names if x not in COLS_TO_KEEP]\n",
    "dataset_train = dataset_train.remove_columns(cols_to_remove)\n",
    "\n",
    "cols_to_remove = [x for x in dataset_val.column_names if x not in COLS_TO_KEEP]\n",
    "dataset_val = dataset_val.remove_columns(cols_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3131d-3f0b-472e-b7a4-dde33eb7828c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = BertConfig.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    output_hidden_states=False,\n",
    "    num_hidden_layers=6,\n",
    "    position_embedding_type='relative_key_query',\n",
    "    classifier_dropout=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774af489-3ff3-4ffa-a572-0bb0484e4d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52206d-4804-4464-a847-10374b476f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = BertForTokenRegression(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072cf8b-90d1-488f-9df5-d8b554aff5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FREEZE_EMBEDDINGS = True\n",
    "INIT_SINUSOIDAL_EMBEDDINGS = True\n",
    "SINUSOIDAL_DISTANCE_EMBEDDINGS=True\n",
    "FREEZE_LAYERS = 0\n",
    "if INIT_SINUSOIDAL_EMBEDDINGS:\n",
    "    print('Sinusoidal embeddings.')\n",
    "    num_pos_emb, emb_size = MODEL.bert.embeddings.position_embeddings.weight.shape\n",
    "    MODEL.bert.embeddings.position_embeddings.weight = torch.nn.Parameter(PositionalEmbedding(emb_size)(torch.arange(0, num_pos_emb)).squeeze(1).clone())\n",
    "if SINUSOIDAL_DISTANCE_EMBEDDINGS:\n",
    "    print('Sinusoidal distance embeddings.')\n",
    "    for layer in MODEL.bert.encoder.layer:\n",
    "        _, emb_size = layer.attention.self.distance_embedding.weight.shape\n",
    "        num_pos_emb = CONFIG.max_position_embeddings\n",
    "        layer.attention.self.distance_embedding.weight = torch.nn.Parameter(PositionalEmbedding(emb_size)(torch.arange(-num_pos_emb + 1, num_pos_emb)).squeeze(1).clone())\n",
    "        layer.attention.self.distance_embedding.requires_grad=False\n",
    "        layer.attention2.self.distance_embedding.weight = torch.nn.Parameter(PositionalEmbedding(emb_size)(torch.arange(-num_pos_emb + 1, num_pos_emb)).squeeze(1).clone())\n",
    "        layer.attention2.self.distance_embedding.requires_grad=False\n",
    "if FREEZE_EMBEDDINGS:\n",
    "    print('Freezing embeddings.')\n",
    "    MODEL.bert.embeddings.position_embeddings.requires_grad=False\n",
    "if FREEZE_LAYERS>0:\n",
    "    print(f'Freezing {FREEZE_LAYERS} layers.')\n",
    "    for layer in MODEL.bert.encoder.layer[:FREEZE_LAYERS]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1352c4-666e-4380-8c28-97d2c438326c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mae(prediction_output: \"PredictionOutput\"):\n",
    "    \n",
    "    predictions_2a3_map = prediction_output.predictions[:,:, 0]\n",
    "    labels_2a3_map = prediction_output.label_ids[:,:, 0]\n",
    "    labels_2a3_map = np.clip(labels_2a3_map, 0, 1)\n",
    "    valid_map = (~np.isnan(labels_2a3_map)) & ~np.isclose(predictions_2a3_map, -100)\n",
    "    mae_2a3_map = np.abs(predictions_2a3_map[valid_map] - labels_2a3_map[valid_map])\n",
    "    mae_2a3_map = np.mean(mae_2a3_map)\n",
    "    \n",
    "    predictions_dms_map = prediction_output.predictions[:,:, 1]\n",
    "    labels_dms_map = prediction_output.label_ids[:,:, 1]\n",
    "    labels_dms_map = np.clip(labels_dms_map, 0, 1)\n",
    "    valid_map = (~np.isnan(labels_dms_map)) & ~np.isclose(predictions_dms_map, -100)\n",
    "    mae_dms_map = np.abs(predictions_dms_map[valid_map] - labels_dms_map[valid_map])\n",
    "    mae_dms_map = np.mean(mae_dms_map)\n",
    "    \n",
    "    mae = (mae_2a3_map + mae_dms_map) / 2\n",
    "    return {'mae': mae, 'mae_2a3_map': mae_2a3_map, 'mae_dms_map': mae_dms_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c848c7-78d4-46cc-9535-39eb4776959d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "class LoggerLRCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        for i, param_group in enumerate(kwargs['optimizer'].param_groups):\n",
    "            # Log the learning rate with a custom key format\n",
    "            logs[f\"lr_{i}\"] = param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff0178-8388-4ff1-a482-fcbfa53daf2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(MODEL.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight', 'DynamicLayerNorm2d.weight', 'DynamicLayerNorm2d.bias', 'BatchNorm1d.weight', 'BatchNorm1d.bias']\n",
    "output_params = ['dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n",
    "lr = 1e-3\n",
    "output_lr = 1e-3\n",
    "weight_decay = 0.05\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and (n not in output_params)], 'weight_decay': weight_decay, 'lr': lr},\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) and (n in output_params)], 'weight_decay': weight_decay, 'lr': output_lr},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and (n not in output_params)], 'weight_decay': 0.0, 'lr': lr},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) and (n in output_params)], 'weight_decay': 0.0, 'lr': output_lr},\n",
    "]\n",
    "optimizer = transformers.AdamW(\n",
    "    optimizer_grouped_parameters\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b333f9e-985b-449e-beb7-23c2e4e8c08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SIGN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0e7ee-08ea-4696-a001-3314d8581cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVAL_STEPS = 1000\n",
    "EVAL_STRATEGY = 'epoch'\n",
    "# assert (EVAL_STRATEGY == 'epoch') == (EVAL_STEPS == None)\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"stanford-ribonanza-rna-folding\", \n",
    "    tags=[\n",
    "        f\"DATASET_LEN_{DATASET_LEN}\",\n",
    "        f\"MAX_LEN_{MAX_LEN}\",\n",
    "        f\"freeze_{FREEZE_LAYERS}\",\n",
    "        f\"freeze__emb_{FREEZE_EMBEDDINGS}\",\n",
    "        f\"sinusoidal_emb_{INIT_SINUSOIDAL_EMBEDDINGS}\",\n",
    "        f\"SINUSOIDAL_DISTANCE_EMBEDDINGS_{SINUSOIDAL_DISTANCE_EMBEDDINGS}\"\n",
    "    ],\n",
    "    group=MODEL_PATH\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=0.1, \n",
    "    # learning_rate=5e-4, # optimizers'lr overrides this\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=15,\n",
    "    report_to='wandb',\n",
    "    output_dir = os.path.join(OUTPUT_DIR, f'checkpoints/{run.id}'),\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=EVAL_STEPS,\n",
    "    evaluation_strategy=EVAL_STRATEGY,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=EVAL_STRATEGY,\n",
    "    save_steps=EVAL_STEPS,\n",
    "    load_best_model_at_end=False,\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model='mae',\n",
    "    lr_scheduler_type='cosine',\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=8,\n",
    "    max_grad_norm=5,\n",
    "    # group_by_length=True,\n",
    "    # length_column_name='len',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=MODEL,\n",
    "    args=training_args,\n",
    "    tokenizer=TOKENIZER,\n",
    "    data_collator=DataCollatorForTokenRegression(tokenizer=TOKENIZER, sign=SIGN),\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    compute_metrics=mae,\n",
    "    optimizers=(optimizer, None),\n",
    "    callbacks=[LoggerLRCallback()]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f'../output/checkpoints/{run.id}/final')\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874fd525-6881-4acd-83d4-d18cb6968a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=MODEL,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForTokenRegression(tokenizer=TOKENIZER, sign=SIGN)\n",
    ")\n",
    "prediction_output = trainer.predict(dataset_val)\n",
    "mae(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c01f35-1e13-4681-b290-47e2d813be5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(\n",
    "    os.path.join(OUTPUT_DIR, f'checkpoints/{run.id}/pseudo_labels_val.npy'),\n",
    "    prediction_output.predictions.transpose(0, 2, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1d25e-c7d4-4104-8319-2774e1868403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.heatmap(DataCollatorForTokenRegression(tokenizer=TOKENIZER, sign=SIGN)([dataset_train[3], dataset_train[100000]])['attention_injection'][0][2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce29a323-70af-4d92-85ff-8beb9a7a38bf",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8925b-8604-41c8-91e4-5f7d72e9fb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_test = load_from_disk(\"../input/stanford-ribonanza-rna-folding/test_sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee551433-b50b-4ac9-a183-2bae72d8be87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_test = dataset_test.map(\n",
    "    preprocess,\n",
    "    fn_kwargs={'tokenizer': TOKENIZER, 'max_len': None},\n",
    "    num_proc=14\n",
    ")\n",
    "\n",
    "dataset_test = dataset_test.map(\n",
    "    run_CapR,\n",
    "    fn_kwargs={\n",
    "        'output_dir': '../output/CapR',\n",
    "        'cache_dir': '../output/CapR_cache',\n",
    "    },\n",
    "    num_proc=14\n",
    ")\n",
    "\n",
    "dataset_test = dataset_test.map(\n",
    "    get_arnie_mfe_distance,\n",
    "    fn_kwargs={\n",
    "        'cache_dir': '../output/arnie/eternafold',\n",
    "        'package': 'eternafold',\n",
    "        'column_suffix': '_mfe_distance',\n",
    "        'file_path_only': True,\n",
    "    },\n",
    "    num_proc=14\n",
    ")\n",
    "\n",
    "dataset_test = dataset_test.map(\n",
    "    get_arnie_predicted_loop_type,\n",
    "    fn_kwargs={\n",
    "        'cache_dir': '../output/arnie/eternafold_bprna',\n",
    "        'package': 'eternafold',\n",
    "        'column_suffix': '_eternafold',\n",
    "    },\n",
    "    num_proc=14\n",
    ")\n",
    "\n",
    "cols_to_remove = [x for x in dataset_test.column_names if x not in (\n",
    "    'input_ids',\n",
    "    'token_type_ids',\n",
    "    'attention_mask',\n",
    "    'file_path_bpp',\n",
    "    'file_path_mfe_distance',\n",
    "    'capr_structure_probs',\n",
    "    'structure_eternafold'\n",
    ")]\n",
    "dataset_test = dataset_test.map(\n",
    "    get_bpps,\n",
    "    fn_kwargs={\n",
    "        'cache_dir': '../output/arnie/eternafold',\n",
    "        'column_suffix': '_bpp',\n",
    "        'file_path_only': True,\n",
    "    },\n",
    "    num_proc=14,\n",
    "    remove_columns=cols_to_remove\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b2f7b-5e6f-4a7b-8e3c-6a79de4390ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_output = trainer.predict(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bfa658-1179-454e-983b-d9fc1c8dcbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(\n",
    "    os.path.join(OUTPUT_DIR, f'checkpoints/{run.id}/pseudo_labels_test.npy'),\n",
    "    prediction_output.predictions.transpose(0, 2, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19edbd82-cdc5-41d1-aa6f-61711275a1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../input/stanford-ribonanza-rna-folding/test_sequences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29288fd1-f134-490e-b4a2-e9260646644c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_DMS_MaP = []\n",
    "result_2A3_MaP = []\n",
    "for index, row in tqdm(df_test.iterrows()):\n",
    "    l = row['id_max'] - row['id_min'] + 1\n",
    "    pred = prediction_output.predictions[index, :, 0].reshape(-1)[:l].tolist()\n",
    "    assert l == len(pred), f'{index}'\n",
    "    result_2A3_MaP += prediction_output.predictions[index, :, 0].reshape(-1)[:l].tolist()\n",
    "    \n",
    "    pred = prediction_output.predictions[index, :, 1].reshape(-1)[:l].tolist()\n",
    "    assert l == len(pred), f'{index}'\n",
    "    result_DMS_MaP += prediction_output.predictions[index, :, 1].reshape(-1)[:l].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3be85-e971-440c-9be3-ca86d5d5f0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submission = pd.read_parquet('../input/stanford-ribonanza-rna-folding/sample_submission.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873635b-c933-4250-8b33-98ac35cbe21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submission['reactivity_DMS_MaP'] = np.array(result_DMS_MaP)\n",
    "df_submission['reactivity_2A3_MaP'] = np.array(result_2A3_MaP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b476f8-92a5-4ab2-ab27-c3de2bab65e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32655a46-1fd5-4f24-a37d-511a822bf533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = os.path.join(OUTPUT_DIR, f'checkpoints/{run.id}/submit.parquet')\n",
    "df_submission.to_parquet(output_path, index=False)\n",
    "\n",
    "# output_path = os.path.join(OUTPUT_DIR, f'checkpoints/{run.id}/submit.csv')\n",
    "# df_submission.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf4998-1b9e-4e2f-bb1c-ff2838bfd4d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo {run.id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050fdf2-4737-4241-8914-ec512a222f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#some parameters\n",
    "font_size=6\n",
    "id1=269545321\n",
    "id2=269724007\n",
    "reshape1=391\n",
    "reshape2=457\n",
    "#get predictions\n",
    "pred_DMS=df_submission[id1:id2+1]['reactivity_DMS_MaP'].to_numpy().reshape(reshape1,reshape2)\n",
    "pred_2A3=df_submission[id1:id2+1]['reactivity_2A3_MaP'].to_numpy().reshape(reshape1,reshape2)\n",
    "#plot mutate and map\n",
    "fig = plt.figure(figsize=(12, 16))\n",
    "plt.subplot(121)\n",
    "plt.title(f'reactivity_DMS_MaP', fontsize=font_size)\n",
    "plt.imshow(pred_DMS,vmin=0,vmax=1, cmap='gray_r')\n",
    "plt.subplot(122)\n",
    "plt.title(f'reactivity_2A3_MaP', fontsize=font_size)\n",
    "plt.imshow(pred_2A3,vmin=0,vmax=1, cmap='gray_r')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4e957-a601-4438-bd73-b1d043707c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c stanford-ribonanza-rna-folding -f {output_path} -m {run.id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0eea1-a0c2-47f0-b627-cf85de50c9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
